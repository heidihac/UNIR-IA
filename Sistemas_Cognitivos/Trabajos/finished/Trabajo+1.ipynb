{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo 1: Introducción a las redes neuronales con TensorFlow y Keras\n",
    "\n",
    "En este primer trabajo, vamos a utilizar una red neuronal para clasificar imágenes de prendas de ropa. Para ello, utilizaremos Keras con TensorFlow.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "El dataset a utilizar es Fashion MNIST, un problema sencillo con imágenes pequeñas de ropa, pero más interesante que el dataset de MNIST. Puedes consultar más información sobre el dataset en [este enlace](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "El código utilizado para contestar tiene que quedar claramente reflejado en el Notebook. Puedes crear nuevas cells si así lo deseas para estructurar tu código y sus salidas. A la hora de entregar el notebook, **asegúrate de que los resultados de ejecutar tu código han quedado guardados** (por ejemplo, a la hora de entrenar una red neuronal tiene que verse claramente un log de los resultados de cada epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure this is reproducable\n",
    "import numpy as np\n",
    "np.random.seed(4422)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(9090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, vamos a obtener los datos. Por suerte para nosotros, estos pueden ser descargados directamente desde Keras, por lo que no tendremos que preocuparnos de tratar con ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acto seguido, normalizamos esos datos de manera similar a como hemos visto con MNIST, obteniendo valores entre 0 y 1. Este paso es muy importante para el correcto funcionamiento de nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Información sobre el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los datos cargados en memoria, vamos a obtener información sobre los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pregunta 1.1 *(0.5 puntos)* ** ¿Cuántas imágenes hay de *training* y de *test*? ¿Qué tamaño tienen las imágenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training images 60000\n",
      "# test images 10000\n",
      "training images size (28, 28)\n"
     ]
    }
   ],
   "source": [
    "### Tu código aquí ###\n",
    "print('# training images {}'.format(x_train.shape[0]))\n",
    "print('# test images {}'.format(x_test.shape[0]))\n",
    "print('training images size {}'.format(x_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta 1.2 *(0.5 puntos)* ** Realizar una exploración de las variables que contienen los datos. Describir en qué consiste un example del dataset (qué información se guarda en cada imagen) y describir qué contiene la información en y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data contains 28*28 pixels of type float64 values between 0.0 and 1.0 for each image.\n",
      "Each image is associated with a single class, have one of {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} labels\n"
     ]
    }
   ],
   "source": [
    "### Tu código aquí ###\n",
    "print('The training data contains 28*28 pixels of type {} values between {} and {} for each image.'.format(x_test.dtype, x_test[0].min(), x_test[0].max()))\n",
    "print('Each image is associated with a single class, have one of {} labels'.format(set(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tu respuesta aquí*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a **visualizar** una imagen de ejemplo. Prueba tu mismo a cambiar la imagen en uso para explorar el dataset visualmente ejecutando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_example(x):\n",
    "    plt.figure()\n",
    "    plt.imshow(x)\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_example(x_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento de una red neuronal simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta 2 *(7 puntos)***. Utilizando Keras, y preparando los datos de X e y como fuera necesario, define y entrena una red neuronal que sea capaz de clasificar imágenes de Fashion MNIST con las siguientes características:\n",
    "\n",
    "* Dos hidden layers de tamaños 128 y 64, utilizando unidades **sigmoid**\n",
    "* Optimizador **sgd**.\n",
    "* Durante el entrenamiento, la red tiene que mostrar resultados de **loss** y **accuracy** por cada epoch.\n",
    "* La red debe entrenar durante **20 epochs** y batch size de **64**.\n",
    "* La última capa debe de ser una capa **softmax**.\n",
    "\n",
    "Tu red tendría que ser capaz de superar fácilmente 60% de accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN\n",
    "Fully connected feed forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "#create model\n",
    "\n",
    "BS=64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 2s - loss: 2.1932 - acc: 0.3900 - val_loss: 2.0309 - val_acc: 0.5004\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "# train model\n",
    "history = model.fit(x_train, y_train, epochs=1, batch_size=BS, verbose=2, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "Just for testing, is not used for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "# Convert to single channel images\n",
    "x_train_single = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test_single = x_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "#create model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(128, kernel_size=3, activation='relu', input_shape=(28,28, 1)))\n",
    "model_cnn.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 11s - loss: 0.7540 - acc: 0.7410 - val_loss: 0.6503 - val_acc: 0.7756\n",
      "Epoch 2/8\n",
      " - 10s - loss: 0.5174 - acc: 0.8175 - val_loss: 0.5444 - val_acc: 0.8026\n",
      "Epoch 3/8\n",
      " - 10s - loss: 0.4605 - acc: 0.8397 - val_loss: 0.4561 - val_acc: 0.8392\n",
      "Epoch 4/8\n",
      " - 10s - loss: 0.4193 - acc: 0.8538 - val_loss: 0.4312 - val_acc: 0.8501\n",
      "Epoch 5/8\n",
      " - 10s - loss: 0.3898 - acc: 0.8638 - val_loss: 0.4751 - val_acc: 0.8174\n",
      "Epoch 6/8\n",
      " - 10s - loss: 0.3674 - acc: 0.8707 - val_loss: 0.3845 - val_acc: 0.8633\n",
      "Epoch 7/8\n",
      " - 10s - loss: 0.3503 - acc: 0.8766 - val_loss: 0.4099 - val_acc: 0.8527\n",
      "Epoch 8/8\n",
      " - 10s - loss: 0.3363 - acc: 0.8804 - val_loss: 0.3951 - val_acc: 0.8540\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "# train model\n",
    "history = model_cnn.fit(x_train_single, y_train, epochs=8, batch_size=BS, verbose=2, validation_data=(x_test_single, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluación del modelo en datos de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado nuestro modelo, vamos a evaluarlo en los datos de test de Fashion MNIST.\n",
    "\n",
    "**Pregunta 3.1 *(1 punto)* **. Utilizando el modelo recién entrenado, obtener la accuracy resultante en el dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5876054244995117\n",
      "Test accuracy: 0.7845\n"
     ]
    }
   ],
   "source": [
    "### Tu código aquí ###\n",
    "score = model.evaluate(x_test_flat, y_test, verbose=2)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta 3.2 *(1 punto)***. Utilizando el método **predict** de Keras, realizar predicciones para los datos de test. Por cada predicción resultante, ¿qué significan los números que obtenemos al hacer predict? ¿Cómo podemos obtener el valor de la clase resultante? (recordar que estamos utilizando una capa softmax para clasificar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction contains an array of 10 elements per sample, indicating the 'probability' of belonging to that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.3383086e-05 7.7273697e-05 1.7262371e-04 1.0021733e-03 1.5283888e-04\n",
      " 1.8514517e-01 1.7083614e-04 2.6829463e-01 1.3332398e-02 5.3159863e-01]\n"
     ]
    }
   ],
   "source": [
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the most likely class for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 ... 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tu respuesta aquí*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
